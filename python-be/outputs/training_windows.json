[
  {
    "index": 1,
    "start": 0.0,
    "end": 6.6,
    "duration": 6.6,
    "text": "Here's a teaser.",
    "guideline_snippets": [
      "Three-Part Learning Framework: **Pattern**: Progressive list reveal for a multi-stage framework.\n**Elements**: Series of `text_overlay` elements with `fade_in_list` animation, each adding a new point to the previous one.\n**Rationale**: Visually reinforces the structured learning path, making it easy for viewers to follow.",
      "Required Fields for All Elements: `timestamp` (number): Th\u1eddi gian xu\u1ea5t hi\u1ec7n c\u1ee7a ph\u1ea7n t\u1eed t\u00ednh b\u1eb1ng gi\u00e2y k\u1ec3 t\u1eeb khi video b\u1eaft \u0111\u1ea7u. Ph\u1ea3i l\u00e0 s\u1ed1 kh\u00f4ng \u00e2m.\n`type` (string): \u0110\u1ecbnh danh lo\u1ea1i ph\u1ea7n t\u1eed (v\u00ed d\u1ee5: `broll`, `text_overlay`, `sound_effect`).\n`layer` (string): L\u1edbp hi\u1ec3n th\u1ecb trong ng\u0103n x\u1ebfp t\u1ed5ng h\u1ee3p.",
      "Practice Principles: **Pattern**: Iconography with brief text overlays for key principles.\n**Elements**: `icon` (overlay layer) with relevant `content` (e.g., lightbulb for \"innovation\"), accompanied by `text_overlay` with `clean_minimal` style.\n**Rationale**: Provides quick visual cues for abstract concepts, enhancing comprehension and retention."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 16,
      "sentence_count": 1,
      "word_count": 3,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 0.45
    }
  },
  {
    "index": 2,
    "start": 6.6,
    "end": 12.64,
    "duration": 6.040000000000001,
    "text": "What if I told you that cures for chronic disease were hidden in our own immune systems?",
    "guideline_snippets": [
      "Changelog: All notable changes to the AI planning knowledge base will be documented in this file.",
      "Content Consistency: Use canonical terminology (\"digital marketing\", \"traditional marketing\", \"B2B/B2C\", \"feature vs benefit\"). *Rationale:* Vocabulary centralised in [glossary.md](glossary.md) ensures consistent messaging.\nKeep tone authoritative, instructional, and encouraging as reflected in both transcripts. *Rationale:* Matches target audience described in video outlines.\nMaintain chronological order in lists (for example, \"Stage 1 -> Stage 2 -> Stage 3\"). *Rationale:* Prevents cognitive dissonance and supports pedagogy.",
      "Recurring Cues: `highlighted_background` remains the default text treatment throughout; maintain consistency unless context demands variation.\nComparisons thrive on side-by-side overlays (for example, \"Feature | Benefit\", \"Organic | Paid\"); ensure columns stay balanced.\nIntroduce new terminology with matching SFX and maintain momentum with responsive motion cues.\nReserve zoom chains for genuine emphasis; interleave with fades or slides to give viewers recovery time."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 88,
      "sentence_count": 1,
      "word_count": 17,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 2.81
    }
  },
  {
    "index": 3,
    "start": 12.64,
    "end": 16.88,
    "duration": 4.239999999999998,
    "text": "If only we could read that information out.",
    "guideline_snippets": [
      "Changelog: All notable changes to the AI planning knowledge base will be documented in this file.",
      "1. Audit Existing Data & Targets: Understand transcript structure, narration pace, and how timeline annotations map to speech.\nCatalogue every element attribute that must be predicted: `timestamp`, `duration`, `type`, `layer`, `content/description`, `style`, `animation`, `sound`, `action`, optional `context` and `confidence` (see [element_schema.json](element_schema.json)).\nConfirm ingestion coverage for both transcripts and JSON timelines; flag missing sections or inconsistent timestamps.",
      "Review Workflow: Automated checks: compare predicted elements to ground truth with metrics (F1, MAE).\nHuman review: run through transcripts while watching the plan, verifying that each cue feels natural.\nRegression guardrail: maintain a curated library of \"golden\" plan outputs; new models must meet or exceed their scores."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 43,
      "sentence_count": 1,
      "word_count": 8,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 1.89
    }
  },
  {
    "index": 4,
    "start": 16.88,
    "end": 20.16,
    "duration": 3.280000000000001,
    "text": "So let's take multiple sclerosis, for example.",
    "guideline_snippets": [
      "Feature vs. Benefit: **Pattern**: Icon effect with a subtle sound cue for emphasis.\n**Elements**: `icon` (overlay layer) appearing with a `ui_pop` `sound_effect` (audio layer) when a feature or benefit is introduced.\n**Rationale**: Draws attention to the distinction and reinforces the learning point with an auditory cue.",
      "6. Evaluation: Quantitative: accuracy/F1 for type classification, MAE for timestamps, BLEU/ROUGE for generated text.\nQualitative: human review against `quality_criteria.md` with focus on narrative fit and layer conflicts.\nStress tests: long speaking segments without elements, rapid-fire sections with many overlays, sections requiring metaphors.",
      "Purpose: The `patterns.json` file provides machine-readable examples that complement the qualitative descriptions in `patterns.md`. It helps the AI model learn:\n**Correct sequencing**: How elements like `text_overlay`, `broll`, and `sound_effect` should follow each other.\n**Contextual application**: When to use specific styles, animations, or sound effects based on the narrative context.\n**Constraint adherence**: Examples that demonstrate compliance with rules defined in `element_schema.json` and `motion_rules.json`."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 46,
      "sentence_count": 1,
      "word_count": 7,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 2.13
    }
  },
  {
    "index": 5,
    "start": 20.16,
    "end": 27.52,
    "duration": 7.359999999999999,
    "text": "Multiple sclerosis is a debilitating autoimmune disease where our own cells attack the brain.",
    "guideline_snippets": [
      "Usage: **Training**: Used as structured training data to teach the AI model how to generate valid and effective video plans.\n**Validation**: Provides concrete test cases for validating the AI's output against known good examples.\n**Debugging**: Helps in understanding and debugging unexpected behaviors in the AI's plan generation by comparing outputs to these reference patterns.\nRefer to `knowledge-base/examples/patterns.md` for qualitative descriptions and further context on these editing patterns.",
      "Changelog: All notable changes to the AI planning knowledge base will be documented in this file.",
      "Purpose: The `patterns.json` file provides machine-readable examples that complement the qualitative descriptions in `patterns.md`. It helps the AI model learn:\n**Correct sequencing**: How elements like `text_overlay`, `broll`, and `sound_effect` should follow each other.\n**Contextual application**: When to use specific styles, animations, or sound effects based on the narrative context.\n**Constraint adherence**: Examples that demonstrate compliance with rules defined in `element_schema.json` and `motion_rules.json`."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 93,
      "sentence_count": 1,
      "word_count": 14,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": true
    },
    "audio_features": {
      "speaking_rate_wps": 1.9
    }
  },
  {
    "index": 6,
    "start": 27.52,
    "end": 33.44,
    "duration": 5.919999999999998,
    "text": "And for years, we didn't know what caused it, which makes it very difficult to treat.",
    "guideline_snippets": [
      "8. Maintenance Cadence: Quarterly data refresh to incorporate new annotated videos.\nMonthly error analysis session targeting repeated failure modes (mis-timed SFX, incorrect layer stacking).\nCapture feedback from editors to expand context tags and example catalogues.",
      "Changelog: All notable changes to the AI planning knowledge base will be documented in this file.",
      "Motion & Transitions: Trigger `zoom_in` on turning points or emphasised statements (\"this is the key\", \"here's the mistake\"). *Rationale:* Heightens focus; validated by examples in [examples/patterns.json](examples/patterns.json).\nDeploy `zoom_out` to release tension or provide wider context after a focal point. *Rationale:* Restores viewer orientation.\nSpace identical effects by at least 0.5 s unless narrative urgency demands otherwise. *Rationale:* Enforced by `motion_rules.json`; avoids viewer fatigue (see negative cases in `patterns.json`)."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 85,
      "sentence_count": 1,
      "word_count": 16,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 2.7
    }
  },
  {
    "index": 7,
    "start": 33.44,
    "end": 42.12,
    "duration": 8.68,
    "text": "So in January 2022, a study came out where they followed 10 million people for 20 years.",
    "guideline_snippets": [
      "Changelog: All notable changes to the AI planning knowledge base will be documented in this file.",
      "8. Maintenance Cadence: Quarterly data refresh to incorporate new annotated videos.\nMonthly error analysis session targeting repeated failure modes (mis-timed SFX, incorrect layer stacking).\nCapture feedback from editors to expand context tags and example catalogues.",
      "Recurring Cues: Lists expand rather than reset, so design overlays that append items sequentially.\nCredibility signals (\"Forbes\", \"Inc\", \"Top 1%\") deserve combined overlay plus b-roll emphasis.\nFailure stories lean on darker-toned visuals and restrained SFX; resolve with lighter shots as tone lifts.\nRelationship-building advice pairs well with collaborative or networking imagery."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 88,
      "sentence_count": 1,
      "word_count": 17,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 1.96
    }
  },
  {
    "index": 8,
    "start": 42.12,
    "end": 47.58,
    "duration": 5.460000000000001,
    "text": "And they found that Epstein-Barr virus, so mono, the kissing disease, increased the risk",
    "guideline_snippets": [
      "Coverage: Every major section (see `videos/*.md`) contains at least one supporting visual element.\nNo unaddressed transcript peaks: stories, statistics, frameworks, and comparisons must trigger at least a text overlay or b-roll.",
      "Layer & Timing Hygiene: Do not overlap full-screen overlays at the same timestamp; offset by at least 0.3 s or merge copy. *Rationale:* Respects the stack hierarchy defined in [element_definitions.md](element_definitions.md#layer-stack).\nWhen the `video` layer is active (b-roll), overlays may occupy full width; otherwise keep margins to protect speaker visibility. *Rationale:* Preserves essential facial expressions.\nEnd SFX before the next major beat and cap overlay visibility at 6-8 s unless the segment is static. *Rationale:* Aligns with [quality_criteria.md](quality_criteria.md#timing) to prevent lingering elements.",
      "Recurring Cues: Lists expand rather than reset, so design overlays that append items sequentially.\nCredibility signals (\"Forbes\", \"Inc\", \"Top 1%\") deserve combined overlay plus b-roll emphasis.\nFailure stories lean on darker-toned visuals and restrained SFX; resolve with lighter shots as tone lifts.\nRelationship-building advice pairs well with collaborative or networking imagery."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 88,
      "sentence_count": 0,
      "word_count": 14,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 2.56
    }
  },
  {
    "index": 9,
    "start": 47.58,
    "end": 50.84,
    "duration": 3.260000000000005,
    "text": "of multiple sclerosis by 32 times.",
    "guideline_snippets": [
      "Layer Integrity: Only one element per layer at any instant (exceptions: audio layer can stack ambience plus SFX if mix-tested).\nTransitions do not collide with overlays unless intentionally coordinated.",
      "8. Maintenance Cadence: Quarterly data refresh to incorporate new annotated videos.\nMonthly error analysis session targeting repeated failure modes (mis-timed SFX, incorrect layer stacking).\nCapture feedback from editors to expand context tags and example catalogues.",
      "Coverage: Every major section (see `videos/*.md`) contains at least one supporting visual element.\nNo unaddressed transcript peaks: stories, statistics, frameworks, and comparisons must trigger at least a text overlay or b-roll."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 34,
      "sentence_count": 1,
      "word_count": 6,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 1.84
    }
  },
  {
    "index": 10,
    "start": 50.84,
    "end": 52.8,
    "duration": 1.9599999999999937,
    "text": "It's a big deal.",
    "guideline_snippets": [
      "Motion & Transitions: Trigger `zoom_in` on turning points or emphasised statements (\"this is the key\", \"here's the mistake\"). *Rationale:* Heightens focus; validated by examples in [examples/patterns.json](examples/patterns.json).\nDeploy `zoom_out` to release tension or provide wider context after a focal point. *Rationale:* Restores viewer orientation.\nSpace identical effects by at least 0.5 s unless narrative urgency demands otherwise. *Rationale:* Enforced by `motion_rules.json`; avoids viewer fatigue (see negative cases in `patterns.json`).",
      "Layer & Timing Hygiene: Do not overlap full-screen overlays at the same timestamp; offset by at least 0.3 s or merge copy. *Rationale:* Respects the stack hierarchy defined in [element_definitions.md](element_definitions.md#layer-stack).\nWhen the `video` layer is active (b-roll), overlays may occupy full width; otherwise keep margins to protect speaker visibility. *Rationale:* Preserves essential facial expressions.\nEnd SFX before the next major beat and cap overlay visibility at 6-8 s unless the segment is static. *Rationale:* Aligns with [quality_criteria.md](quality_criteria.md#timing) to prevent lingering elements.",
      "Sound Effects: Pair new terms or section changes with subtle SFX (`ui_pop`, `whoosh_standard`). *Rationale:* Audio cues boost recall and correspond to assets in `sfx_catalog.json`.\nReserve `money`, `success`, or `achievement` sounds for monetary claims or milestones. *Rationale:* Prevents semantic drift and keeps the acoustic palette purposeful.\nUse `typing` sounds only when animations or narration mention writing or typing. *Rationale:* Maintains cohesive audiovisual storytelling."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 16,
      "sentence_count": 1,
      "word_count": 4,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 2.04
    }
  },
  {
    "index": 11,
    "start": 52.8,
    "end": 55.44,
    "duration": 2.6400000000000006,
    "text": "You might have seen it in the news.",
    "guideline_snippets": [
      "Coverage: Every major section (see `videos/*.md`) contains at least one supporting visual element.\nNo unaddressed transcript peaks: stories, statistics, frameworks, and comparisons must trigger at least a text overlay or b-roll.",
      "Motion & Transitions: Trigger `zoom_in` on turning points or emphasised statements (\"this is the key\", \"here's the mistake\"). *Rationale:* Heightens focus; validated by examples in [examples/patterns.json](examples/patterns.json).\nDeploy `zoom_out` to release tension or provide wider context after a focal point. *Rationale:* Restores viewer orientation.\nSpace identical effects by at least 0.5 s unless narrative urgency demands otherwise. *Rationale:* Enforced by `motion_rules.json`; avoids viewer fatigue (see negative cases in `patterns.json`).",
      "8. Maintenance Cadence: Quarterly data refresh to incorporate new annotated videos.\nMonthly error analysis session targeting repeated failure modes (mis-timed SFX, incorrect layer stacking).\nCapture feedback from editors to expand context tags and example catalogues."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 35,
      "sentence_count": 1,
      "word_count": 8,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 3.03
    }
  },
  {
    "index": 12,
    "start": 55.44,
    "end": 61.28,
    "duration": 5.840000000000003,
    "text": "So three days later, another study came out in nature showing not just a correlation,",
    "guideline_snippets": [
      "Coverage: Every major section (see `videos/*.md`) contains at least one supporting visual element.\nNo unaddressed transcript peaks: stories, statistics, frameworks, and comparisons must trigger at least a text overlay or b-roll.",
      "Recurring Cues: Lists expand rather than reset, so design overlays that append items sequentially.\nCredibility signals (\"Forbes\", \"Inc\", \"Top 1%\") deserve combined overlay plus b-roll emphasis.\nFailure stories lean on darker-toned visuals and restrained SFX; resolve with lighter shots as tone lifts.\nRelationship-building advice pairs well with collaborative or networking imagery.",
      "8. Maintenance Cadence: Quarterly data refresh to incorporate new annotated videos.\nMonthly error analysis session targeting repeated failure modes (mis-timed SFX, incorrect layer stacking).\nCapture feedback from editors to expand context tags and example catalogues."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 85,
      "sentence_count": 0,
      "word_count": 15,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 2.57
    }
  },
  {
    "index": 13,
    "start": 61.28,
    "end": 68.92,
    "duration": 7.640000000000001,
    "text": "but a direct link between Epstein-Barr virus and multiple sclerosis, using just 12 people",
    "guideline_snippets": [
      "Coverage: Every major section (see `videos/*.md`) contains at least one supporting visual element.\nNo unaddressed transcript peaks: stories, statistics, frameworks, and comparisons must trigger at least a text overlay or b-roll.",
      "Recurring Cues: Lists expand rather than reset, so design overlays that append items sequentially.\nCredibility signals (\"Forbes\", \"Inc\", \"Top 1%\") deserve combined overlay plus b-roll emphasis.\nFailure stories lean on darker-toned visuals and restrained SFX; resolve with lighter shots as tone lifts.\nRelationship-building advice pairs well with collaborative or networking imagery.",
      "Layer Integrity: Only one element per layer at any instant (exceptions: audio layer can stack ambience plus SFX if mix-tested).\nTransitions do not collide with overlays unless intentionally coordinated."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 89,
      "sentence_count": 0,
      "word_count": 14,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 1.83
    }
  },
  {
    "index": 14,
    "start": 68.92,
    "end": 71.84,
    "duration": 2.9200000000000017,
    "text": "and sampling like once.",
    "guideline_snippets": [
      "9. Risk Controls: Mitigate noisy timestamps by smoothing predictions and enforcing minimum separation thresholds defined in `motion_rules.json`.\nPrevent style drift by constraining `style` predictions to allowed set per brand guide.\nDocument assumptions about transcript accuracy; include fallbacks when speech deviates from script.",
      "8. Maintenance Cadence: Quarterly data refresh to incorporate new annotated videos.\nMonthly error analysis session targeting repeated failure modes (mis-timed SFX, incorrect layer stacking).\nCapture feedback from editors to expand context tags and example catalogues.",
      "5. Training Routine: Split data by video to avoid leakage (train on one video, validate on another, rotate as more data arrives).\nUse curriculum: start with type/layer predictions, then add auxiliary heads (content, style).\nApply class balancing (oversampling rare elements like `achievement_highlight`).\nMonitor convergence per head; early-stop if accuracy plateaus but loss diverges."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 23,
      "sentence_count": 1,
      "word_count": 4,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 1.37
    }
  },
  {
    "index": 15,
    "start": 71.84,
    "end": 79.94,
    "duration": 8.099999999999994,
    "text": "So 10 million people, 20 years, 12 people in like a day.",
    "guideline_snippets": [
      "4. Model Strategy: Treat timestamp and duration predictions as regression; element presence and type as multi-label or multi-class classification.\nConsider hierarchical modelling: first detect whether an element occurs, then specialise by element family.\nExplore sequence-to-sequence or encoder-decoder approaches for generating ordered element lists.\nReserve capacity for generative text (for example, overlay copy) via template libraries or conditional generation models.",
      "8. Maintenance Cadence: Quarterly data refresh to incorporate new annotated videos.\nMonthly error analysis session targeting repeated failure modes (mis-timed SFX, incorrect layer stacking).\nCapture feedback from editors to expand context tags and example catalogues.",
      "2. Preprocess & Align: Merge transcript text with timeline entries by timestamp, producing windows (5-10 seconds) that give sufficient context.\nNormalize timestamps to seconds for easier maths and modelling.\nSegment transcripts into sentences or clauses; tokenise and build embeddings (Sentence-BERT or equivalent).\nEncode video elements: one-hot for categorical fields, embeddings for text-heavy attributes (`description`, `content`); ensure categorical vocab matches [element_schema.json](element_schema.json)."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 56,
      "sentence_count": 1,
      "word_count": 12,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 1.48
    }
  },
  {
    "index": 16,
    "start": 79.94,
    "end": 82.16,
    "duration": 2.219999999999999,
    "text": "How did they do that?",
    "guideline_snippets": [
      "9. Risk Controls: Mitigate noisy timestamps by smoothing predictions and enforcing minimum separation thresholds defined in `motion_rules.json`.\nPrevent style drift by constraining `style` predictions to allowed set per brand guide.\nDocument assumptions about transcript accuracy; include fallbacks when speech deviates from script.",
      "8. Maintenance Cadence: Quarterly data refresh to incorporate new annotated videos.\nMonthly error analysis session targeting repeated failure modes (mis-timed SFX, incorrect layer stacking).\nCapture feedback from editors to expand context tags and example catalogues.",
      "Review Workflow: Automated checks: compare predicted elements to ground truth with metrics (F1, MAE).\nHuman review: run through transcripts while watching the plan, verifying that each cue feels natural.\nRegression guardrail: maintain a curated library of \"golden\" plan outputs; new models must meet or exceed their scores."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 21,
      "sentence_count": 1,
      "word_count": 5,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 2.25
    }
  },
  {
    "index": 17,
    "start": 82.16,
    "end": 88.72,
    "duration": 6.560000000000002,
    "text": "So the team, Robinson and Lance, used a snapshot of information stored in our memory immune",
    "guideline_snippets": [
      "Extraction Reminders: Keep transcript timestamps in `M:SS` and convert to seconds during preprocessing.\nPreserve `context`, `style`, `animation`, and `sound` attributes; they contain supervision signals even when absent from transcripts.\nWhen augmenting data, cross-link transcript spans to timeline entries via timestamp proximity (+/- 1 second window works for both videos).\nTrack narrative sections (e.g., `Part 1`, `B2B vs B2C`) to help the model learn context-aware element placement.\nSynchronise with asset catalogs so generated descriptions map to real `id`s and comply with motion spacing rules.",
      "1. Audit Existing Data & Targets: Understand transcript structure, narration pace, and how timeline annotations map to speech.\nCatalogue every element attribute that must be predicted: `timestamp`, `duration`, `type`, `layer`, `content/description`, `style`, `animation`, `sound`, `action`, optional `context` and `confidence` (see [element_schema.json](element_schema.json)).\nConfirm ingestion coverage for both transcripts and JSON timelines; flag missing sections or inconsistent timestamps.",
      "Changelog: All notable changes to the AI planning knowledge base will be documented in this file."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 91,
      "sentence_count": 0,
      "word_count": 16,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 2.44
    }
  },
  {
    "index": 18,
    "start": 88.72,
    "end": 94.96,
    "duration": 6.239999999999995,
    "text": "cells to infer backwards what had come before and triggered the disease, or what I like",
    "guideline_snippets": [
      "Changelog: All notable changes to the AI planning knowledge base will be documented in this file.",
      "1. Audit Existing Data & Targets: Understand transcript structure, narration pace, and how timeline annotations map to speech.\nCatalogue every element attribute that must be predicted: `timestamp`, `duration`, `type`, `layer`, `content/description`, `style`, `animation`, `sound`, `action`, optional `context` and `confidence` (see [element_schema.json](element_schema.json)).\nConfirm ingestion coverage for both transcripts and JSON timelines; flag missing sections or inconsistent timestamps.",
      "Coverage: Every major section (see `videos/*.md`) contains at least one supporting visual element.\nNo unaddressed transcript peaks: stories, statistics, frameworks, and comparisons must trigger at least a text overlay or b-roll."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 87,
      "sentence_count": 0,
      "word_count": 16,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 2.56
    }
  },
  {
    "index": 19,
    "start": 94.96,
    "end": 99.72,
    "duration": 4.760000000000005,
    "text": "to call forensic immunology.",
    "guideline_snippets": [
      "1. Audit Existing Data & Targets: Understand transcript structure, narration pace, and how timeline annotations map to speech.\nCatalogue every element attribute that must be predicted: `timestamp`, `duration`, `type`, `layer`, `content/description`, `style`, `animation`, `sound`, `action`, optional `context` and `confidence` (see [element_schema.json](element_schema.json)).\nConfirm ingestion coverage for both transcripts and JSON timelines; flag missing sections or inconsistent timestamps.",
      "Usage: **Training**: Used as structured training data to teach the AI model how to generate valid and effective video plans.\n**Validation**: Provides concrete test cases for validating the AI's output against known good examples.\n**Debugging**: Helps in understanding and debugging unexpected behaviors in the AI's plan generation by comparing outputs to these reference patterns.\nRefer to `knowledge-base/examples/patterns.md` for qualitative descriptions and further context on these editing patterns.",
      "Layer Stack: `main` - primary camera feed (baseline footage).\n`video` - b-roll replacing or augmenting the main layer.\n`overlay` - text, icons, and animated graphics.\n`audio` - sound effects mixed with narration.\n`transition` - temporary visual effects used between clips.\nRespect layer priority to avoid stacking conflicts; only one dominant element per layer at a time."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 28,
      "sentence_count": 1,
      "word_count": 4,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 0.84
    }
  },
  {
    "index": 20,
    "start": 99.72,
    "end": 104.08,
    "duration": 4.359999999999999,
    "text": "And in Robinson's case, right, they already knew what they were looking for, Epstein-Barr",
    "guideline_snippets": [
      "Narrative Alignment: Anchor every element to a transcript clause; avoid floating overlays with no spoken support. *Rationale:* Maintains coherence with dialogue and prevents disjointed visuals.\nTrack signal phrases (\"Part 1\", \"Stage 2\", \"Option 3\", \"Here's why\") to justify section headers or text overlays. *Rationale:* Reinforces structural beats described in [videos/video1_outline.md](videos/video1_outline.md) and [videos/video2_outline.md](videos/video2_outline.md).\nAlign stories and analogies with metaphorical b-roll using `context` hints or tag matches from [asset_catalogs.md](asset_catalogs.md). *Rationale:* Visualises abstract concepts for faster comprehension.",
      "Coverage: Every major section (see `videos/*.md`) contains at least one supporting visual element.\nNo unaddressed transcript peaks: stories, statistics, frameworks, and comparisons must trigger at least a text overlay or b-roll.",
      "Sound Effects: Pair new terms or section changes with subtle SFX (`ui_pop`, `whoosh_standard`). *Rationale:* Audio cues boost recall and correspond to assets in `sfx_catalog.json`.\nReserve `money`, `success`, or `achievement` sounds for monetary claims or milestones. *Rationale:* Prevents semantic drift and keeps the acoustic palette purposeful.\nUse `typing` sounds only when animations or narration mention writing or typing. *Rationale:* Maintains cohesive audiovisual storytelling."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 89,
      "sentence_count": 0,
      "word_count": 14,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 3.21
    }
  },
  {
    "index": 21,
    "start": 104.08,
    "end": 106.32,
    "duration": 2.239999999999995,
    "text": "virus, myelin.",
    "guideline_snippets": [
      "Introduction & Hook: **Pattern**: Speaker intro with a bold, engaging question.\n**Elements**: `speaker_intro` (main layer), `text_overlay` (overlay layer) with `bold_emphasis` style.\n**Rationale**: Immediately captures viewer attention and sets the stage for the video's core topic.",
      "Practice Principles: **Pattern**: Iconography with brief text overlays for key principles.\n**Elements**: `icon` (overlay layer) with relevant `content` (e.g., lightbulb for \"innovation\"), accompanied by `text_overlay` with `clean_minimal` style.\n**Rationale**: Provides quick visual cues for abstract concepts, enhancing comprehension and retention.",
      "Feature vs. Benefit: **Pattern**: Icon effect with a subtle sound cue for emphasis.\n**Elements**: `icon` (overlay layer) appearing with a `ui_pop` `sound_effect` (audio layer) when a feature or benefit is introduced.\n**Rationale**: Draws attention to the distinction and reinforces the learning point with an auditory cue."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 14,
      "sentence_count": 1,
      "word_count": 2,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 0.89
    }
  },
  {
    "index": 22,
    "start": 106.32,
    "end": 114.64,
    "duration": 8.320000000000007,
    "text": "So they only had to decode a tiny part of the massive, distributed, ever-evolving archive",
    "guideline_snippets": [
      "Coverage: Every major section (see `videos/*.md`) contains at least one supporting visual element.\nNo unaddressed transcript peaks: stories, statistics, frameworks, and comparisons must trigger at least a text overlay or b-roll.",
      "1. Audit Existing Data & Targets: Understand transcript structure, narration pace, and how timeline annotations map to speech.\nCatalogue every element attribute that must be predicted: `timestamp`, `duration`, `type`, `layer`, `content/description`, `style`, `animation`, `sound`, `action`, optional `context` and `confidence` (see [element_schema.json](element_schema.json)).\nConfirm ingestion coverage for both transcripts and JSON timelines; flag missing sections or inconsistent timestamps.",
      "2. Preprocess & Align: Merge transcript text with timeline entries by timestamp, producing windows (5-10 seconds) that give sufficient context.\nNormalize timestamps to seconds for easier maths and modelling.\nSegment transcripts into sentences or clauses; tokenise and build embeddings (Sentence-BERT or equivalent).\nEncode video elements: one-hot for categorical fields, embeddings for text-heavy attributes (`description`, `content`); ensure categorical vocab matches [element_schema.json](element_schema.json)."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 89,
      "sentence_count": 0,
      "word_count": 15,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 1.8
    }
  },
  {
    "index": 23,
    "start": 114.64,
    "end": 117.52,
    "duration": 2.8799999999999955,
    "text": "that is the memory immune system.",
    "guideline_snippets": [
      "Changelog: All notable changes to the AI planning knowledge base will be documented in this file.",
      "Feature vs. Benefit: **Pattern**: Icon effect with a subtle sound cue for emphasis.\n**Elements**: `icon` (overlay layer) appearing with a `ui_pop` `sound_effect` (audio layer) when a feature or benefit is introduced.\n**Rationale**: Draws attention to the distinction and reinforces the learning point with an auditory cue.",
      "Three-Part Learning Framework: **Pattern**: Progressive list reveal for a multi-stage framework.\n**Elements**: Series of `text_overlay` elements with `fade_in_list` animation, each adding a new point to the previous one.\n**Rationale**: Visually reinforces the structured learning path, making it easy for viewers to follow."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 33,
      "sentence_count": 1,
      "word_count": 6,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 2.08
    }
  },
  {
    "index": 24,
    "start": 117.52,
    "end": 123.52,
    "duration": 6.0,
    "text": "So the immune system stores an imprint of everything it encounters, infections, allergies,",
    "guideline_snippets": [
      "Purpose: The `patterns.json` file provides machine-readable examples that complement the qualitative descriptions in `patterns.md`. It helps the AI model learn:\n**Correct sequencing**: How elements like `text_overlay`, `broll`, and `sound_effect` should follow each other.\n**Contextual application**: When to use specific styles, animations, or sound effects based on the narrative context.\n**Constraint adherence**: Examples that demonstrate compliance with rules defined in `element_schema.json` and `motion_rules.json`.",
      "Changelog: All notable changes to the AI planning knowledge base will be documented in this file.",
      "Structure: The `patterns.json` file is an array of objects, where each object represents a specific editing pattern. Each pattern typically includes:\n**`name`**: A descriptive name for the pattern.\n**`description`**: A brief explanation of the pattern's intent and usage.\n**`elements`**: An array of video elements, each conforming to the `element_schema.json`, demonstrating the pattern. These elements include `timestamp`, `type`, `layer`, and other relevant properties.\n**`notes`**: Additional qualitative notes or rationale for the pattern."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 90,
      "sentence_count": 0,
      "word_count": 13,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 2.17
    }
  },
  {
    "index": 25,
    "start": 123.52,
    "end": 127.72,
    "duration": 4.200000000000003,
    "text": "autoimmunity, toxins, cancer.",
    "guideline_snippets": [
      "6. Evaluation: Quantitative: accuracy/F1 for type classification, MAE for timestamps, BLEU/ROUGE for generated text.\nQualitative: human review against `quality_criteria.md` with focus on narrative fit and layer conflicts.\nStress tests: long speaking segments without elements, rapid-fire sections with many overlays, sections requiring metaphors.",
      "1. Audit Existing Data & Targets: Understand transcript structure, narration pace, and how timeline annotations map to speech.\nCatalogue every element attribute that must be predicted: `timestamp`, `duration`, `type`, `layer`, `content/description`, `style`, `animation`, `sound`, `action`, optional `context` and `confidence` (see [element_schema.json](element_schema.json)).\nConfirm ingestion coverage for both transcripts and JSON timelines; flag missing sections or inconsistent timestamps.",
      "3. Feature Engineering: Transcript features: semantic vectors, sentence length, relative position within the video, keyword flags (for example, \"Part 1\", \"mistake\", \"option\").\nAudio/tempo proxies: estimate speaking rate via word counts per window.\nTimeline context: distance to previous/next elements, element type history, context tags (aligned to `context_rules.json`).\nStyle cues: reuse `context` labels to signal metaphors, comparisons, or emphasis needs.\nAsset compatibility: embed catalog metadata (for example, `tags`, `mood`, `intensity`) from `broll_catalog.json` and `sfx_catalog.json` for recommendation scoring."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 29,
      "sentence_count": 1,
      "word_count": 3,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 0.71
    }
  },
  {
    "index": 26,
    "start": 127.72,
    "end": 131.0,
    "duration": 3.280000000000001,
    "text": "It's basically a web browser history.",
    "guideline_snippets": [
      "Structure: The `patterns.json` file is an array of objects, where each object represents a specific editing pattern. Each pattern typically includes:\n**`name`**: A descriptive name for the pattern.\n**`description`**: A brief explanation of the pattern's intent and usage.\n**`elements`**: An array of video elements, each conforming to the `element_schema.json`, demonstrating the pattern. These elements include `timestamp`, `type`, `layer`, and other relevant properties.\n**`notes`**: Additional qualitative notes or rationale for the pattern.",
      "3. Feature Engineering: Transcript features: semantic vectors, sentence length, relative position within the video, keyword flags (for example, \"Part 1\", \"mistake\", \"option\").\nAudio/tempo proxies: estimate speaking rate via word counts per window.\nTimeline context: distance to previous/next elements, element type history, context tags (aligned to `context_rules.json`).\nStyle cues: reuse `context` labels to signal metaphors, comparisons, or emphasis needs.\nAsset compatibility: embed catalog metadata (for example, `tags`, `mood`, `intensity`) from `broll_catalog.json` and `sfx_catalog.json` for recommendation scoring.",
      "Three-Part Learning Framework: **Pattern**: Progressive list reveal for a multi-stage framework.\n**Elements**: Series of `text_overlay` elements with `fade_in_list` animation, each adding a new point to the previous one.\n**Rationale**: Visually reinforces the structured learning path, making it easy for viewers to follow."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 37,
      "sentence_count": 1,
      "word_count": 6,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 1.83
    }
  },
  {
    "index": 27,
    "start": 131.0,
    "end": 133.72,
    "duration": 2.719999999999999,
    "text": "It's also how vaccines work.",
    "guideline_snippets": [
      "Feature vs. Benefit: **Pattern**: Icon effect with a subtle sound cue for emphasis.\n**Elements**: `icon` (overlay layer) appearing with a `ui_pop` `sound_effect` (audio layer) when a feature or benefit is introduced.\n**Rationale**: Draws attention to the distinction and reinforces the learning point with an auditory cue.",
      "Channel Definitions: **Pattern**: Split-column text overlay for comparing different channels.\n**Elements**: `text_overlay` (overlay layer) with `split_column` style, presenting two or more channels side-by-side.\n**Rationale**: Facilitates direct comparison and highlights distinctions between various digital marketing channels.",
      "Usage: **Training**: Used as structured training data to teach the AI model how to generate valid and effective video plans.\n**Validation**: Provides concrete test cases for validating the AI's output against known good examples.\n**Debugging**: Helps in understanding and debugging unexpected behaviors in the AI's plan generation by comparing outputs to these reference patterns.\nRefer to `knowledge-base/examples/patterns.md` for qualitative descriptions and further context on these editing patterns."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 28,
      "sentence_count": 1,
      "word_count": 5,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 1.84
    }
  },
  {
    "index": 28,
    "start": 133.72,
    "end": 140.8,
    "duration": 7.0800000000000125,
    "text": "So though we may not know what is causing a disease, the immune system probably does.",
    "guideline_snippets": [
      "Changelog: All notable changes to the AI planning knowledge base will be documented in this file.",
      "Coverage: Every major section (see `videos/*.md`) contains at least one supporting visual element.\nNo unaddressed transcript peaks: stories, statistics, frameworks, and comparisons must trigger at least a text overlay or b-roll.",
      "Purpose: The `patterns.json` file provides machine-readable examples that complement the qualitative descriptions in `patterns.md`. It helps the AI model learn:\n**Correct sequencing**: How elements like `text_overlay`, `broll`, and `sound_effect` should follow each other.\n**Contextual application**: When to use specific styles, animations, or sound effects based on the narrative context.\n**Constraint adherence**: Examples that demonstrate compliance with rules defined in `element_schema.json` and `motion_rules.json`."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 85,
      "sentence_count": 1,
      "word_count": 16,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 2.26
    }
  },
  {
    "index": 29,
    "start": 140.8,
    "end": 146.92,
    "duration": 6.119999999999976,
    "text": "And if we could decode the immune archive at scale, we could apply forensic immunology",
    "guideline_snippets": [
      "Coverage: Every major section (see `videos/*.md`) contains at least one supporting visual element.\nNo unaddressed transcript peaks: stories, statistics, frameworks, and comparisons must trigger at least a text overlay or b-roll.",
      "1. Audit Existing Data & Targets: Understand transcript structure, narration pace, and how timeline annotations map to speech.\nCatalogue every element attribute that must be predicted: `timestamp`, `duration`, `type`, `layer`, `content/description`, `style`, `animation`, `sound`, `action`, optional `context` and `confidence` (see [element_schema.json](element_schema.json)).\nConfirm ingestion coverage for both transcripts and JSON timelines; flag missing sections or inconsistent timestamps.",
      "2. Preprocess & Align: Merge transcript text with timeline entries by timestamp, producing windows (5-10 seconds) that give sufficient context.\nNormalize timestamps to seconds for easier maths and modelling.\nSegment transcripts into sentences or clauses; tokenise and build embeddings (Sentence-BERT or equivalent).\nEncode video elements: one-hot for categorical fields, embeddings for text-heavy attributes (`description`, `content`); ensure categorical vocab matches [element_schema.json](element_schema.json)."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 86,
      "sentence_count": 0,
      "word_count": 15,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 2.45
    }
  },
  {
    "index": 30,
    "start": 146.92,
    "end": 150.64,
    "duration": 3.719999999999999,
    "text": "to diseases where we don't yet know the cause.",
    "guideline_snippets": [
      "Usage: **Training**: Used as structured training data to teach the AI model how to generate valid and effective video plans.\n**Validation**: Provides concrete test cases for validating the AI's output against known good examples.\n**Debugging**: Helps in understanding and debugging unexpected behaviors in the AI's plan generation by comparing outputs to these reference patterns.\nRefer to `knowledge-base/examples/patterns.md` for qualitative descriptions and further context on these editing patterns.",
      "Changelog: All notable changes to the AI planning knowledge base will be documented in this file.",
      "Feature vs. Benefit: **Pattern**: Icon effect with a subtle sound cue for emphasis.\n**Elements**: `icon` (overlay layer) appearing with a `ui_pop` `sound_effect` (audio layer) when a feature or benefit is introduced.\n**Rationale**: Draws attention to the distinction and reinforces the learning point with an auditory cue."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 46,
      "sentence_count": 1,
      "word_count": 9,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 2.42
    }
  },
  {
    "index": 31,
    "start": 150.64,
    "end": 154.28,
    "duration": 3.640000000000015,
    "text": "So that is exactly what my team aims to do.",
    "guideline_snippets": [
      "Usage Guidelines: **Match by Context Tags** - ensure each generated element includes `context` (see `element_schema.json`). Use it to query `context_rules.json` and retrieve permitted `broll` or `sfx` tags.\n**Validate Asset Availability** - before finalising a plan, confirm `description` or `sound` values map to existing `id`s in the relevant catalog; log fallback suggestions when no exact match is found.\n**Respect Motion Constraints** - defer to `motion_rules.json` for allowable sequences (for example, minimum 0.5 s spacing between identical zooms).\n**Enrich Training Examples** - augment training data with catalog metadata (for example, embed `mood` vectors) so the model learns to pick assets aligned with emotional tone.\n**Keep Catalogs Synced** - update version numbers and regeneration timestamps when assets change; propagate updates to downstream caches.",
      "Review Workflow: Automated checks: compare predicted elements to ground truth with metrics (F1, MAE).\nHuman review: run through transcripts while watching the plan, verifying that each cue feels natural.\nRegression guardrail: maintain a curated library of \"golden\" plan outputs; new models must meet or exceed their scores.",
      "Asset Catalog Integration: This document details how asset catalogs are integrated into the video automation pipeline. It ensures that the AI model's outputs can be directly mapped to available production assets, guaranteeing the generation of actionable and feasible video plans.\nConnect model outputs with available production assets to guarantee actionable plans."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 43,
      "sentence_count": 1,
      "word_count": 10,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": true
    },
    "audio_features": {
      "speaking_rate_wps": 2.75
    }
  },
  {
    "index": 32,
    "start": 154.28,
    "end": 162.36,
    "duration": 8.080000000000013,
    "text": "And so I am thrilled to announce publicly for the first time, Imprint, a forensic immunology",
    "guideline_snippets": [
      "Coverage: Every major section (see `videos/*.md`) contains at least one supporting visual element.\nNo unaddressed transcript peaks: stories, statistics, frameworks, and comparisons must trigger at least a text overlay or b-roll.",
      "1. Audit Existing Data & Targets: Understand transcript structure, narration pace, and how timeline annotations map to speech.\nCatalogue every element attribute that must be predicted: `timestamp`, `duration`, `type`, `layer`, `content/description`, `style`, `animation`, `sound`, `action`, optional `context` and `confidence` (see [element_schema.json](element_schema.json)).\nConfirm ingestion coverage for both transcripts and JSON timelines; flag missing sections or inconsistent timestamps.",
      "Integration Touchpoints: **Feature Engineering** - include catalog tags as additional inputs when predicting `description` or `sound`.\n**Inference** - post-process model outputs by selecting the closest matching `id` using cosine similarity between description embeddings and catalog tags.\n**Evaluation** - add assertions in `quality_criteria.md` to confirm selected assets exist and comply with `motion_rules.json`.\nRefer back to `knowledge-base/data_sources.md` for provenance and to `planning_guidelines.md` for contextual placement rules."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 92,
      "sentence_count": 0,
      "word_count": 16,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 1.98
    }
  },
  {
    "index": 33,
    "start": 162.36,
    "end": 168.36,
    "duration": 6.0,
    "text": "focused research organization, or FRO, building machine learning and experimental tools to",
    "guideline_snippets": [
      "Review Workflow: Automated checks: compare predicted elements to ground truth with metrics (F1, MAE).\nHuman review: run through transcripts while watching the plan, verifying that each cue feels natural.\nRegression guardrail: maintain a curated library of \"golden\" plan outputs; new models must meet or exceed their scores.",
      "Changelog: All notable changes to the AI planning knowledge base will be documented in this file.",
      "Glossary: This document provides a comprehensive glossary of key terms and concepts used throughout the AI video automation project. Definitions are grounded in the reference transcripts, training documentation, and general machine learning terminology. Understanding these terms is essential for navigating the project's codebase and documentation effectively.\nShort definitions grounded in the reference transcripts and training documentation.\n**Digital marketing** - marketing activities executed through digital channels (SEO, social media, PPC, email, websites). *Related:* Traditional marketing.\n**Traditional marketing** - offline channels such as print, radio, direct mail; contrasted with digital to highlight reach and measurability differences. *See also:* [planning_guidelines.md](planning_guidelines.md#narrative-alignment).\n**SEO (Search Engine Optimization)** - practice of improving organic visibility; central focus of the speaker's career pivot in Video 1. *Related:* Search marketing.\n**B-roll** - supplemental footage used to visualise or emphasise narration. *See also:* [asset_catalogs.md](asset_catalogs.md#catalog-overview).\n**Callout / Highlight** - on-screen emphasis for key phrases, statistics, or warnings, typically rendered as `text_overlay`.\n**Framework** - structured list or set of stages (for example, three-part learning model, four practice principles); often represented with progressive overlays.\n**Stage 1 / 2 / 3 (Learning)** - understand fundamentals -> connect fundamentals -> practise execution (Video 1).\n**Product marketing** - showcasing features and tangible benefits of a physical good.\n**Service marketing** - selling the outcome or end state (experience, trust, transformation) rather than the service mechanics.\n**B2B (Business to Business)** - companies selling to other businesses; longer cycles, committee decisions.\n**B2C (Business to Consumer)** - companies selling directly to consumers; faster decisions, emotion-driven.\n**Feature vs Benefit** - feature describes what something is; benefit explains the value it delivers (Video 2 pen example).\n**Trust signals** - elements that reinforce credibility (awards, experience, social proof).\n**Motion cue** - visual effect (zoom, slide) signalling a narrative shift or emphasis. *See also:* Motion rules in `motion_rules.json`.\n**Embedding** - numeric vector representing semantic meaning of text or assets, used for similarity search and modelling (for example, Sentence-BERT). *Related:* Feature engineering.\n**Word2Vec / GloVe / Sentence-BERT** - families of embedding models; Sentence-BERT produces sentence-level vectors well suited for transcript windows.\n**One-hot encoding** - representation where each category maps to a unique vector with a single 1 and the rest 0; used for element types and styles.\n**Regression head** - model component predicting continuous values (for example, `timestamp`, `duration`). *See:* `training_pipeline.md`.\n**Multi-label classification** - predicting multiple boolean outcomes simultaneously (for example, presence of several element families in a window).\n**MAE (Mean Absolute Error)** - evaluation metric for regression measuring average absolute difference between predicted and true values.\n**F1 score** - harmonic mean of precision and recall; used to assess classification performance.\n**BLEU / ROUGE** - text generation metrics comparing model output to reference wording; apply to overlay copy evaluation.\n**Early stopping** - training technique that halts optimisation when validation performance stops improving to avoid overfitting.\n**Curriculum learning** - training strategy that teaches simpler tasks before harder ones (for example, predict type before content).\n**Confidence score** - probability estimate output by the model indicating reliability of a prediction; captured in `element_schema.json`.\n**Layer stack** - render order from `main` -> `video` -> `overlay` -> `audio` -> `transition`; detailed in [element_definitions.md](element_definitions.md#layer-stack).\n**Context tag** - short descriptive string indicating narrative intent (for example, \"mistake warning\"); used to query asset catalogs and maintain coherence."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 90,
      "sentence_count": 0,
      "word_count": 12,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 2.0
    }
  },
  {
    "index": 34,
    "start": 168.36,
    "end": 173.52,
    "duration": 5.159999999999997,
    "text": "identify the hidden causes of and cures for chronic disease.",
    "guideline_snippets": [
      "6. Evaluation: Quantitative: accuracy/F1 for type classification, MAE for timestamps, BLEU/ROUGE for generated text.\nQualitative: human review against `quality_criteria.md` with focus on narrative fit and layer conflicts.\nStress tests: long speaking segments without elements, rapid-fire sections with many overlays, sections requiring metaphors.",
      "Content Consistency: Use canonical terminology (\"digital marketing\", \"traditional marketing\", \"B2B/B2C\", \"feature vs benefit\"). *Rationale:* Vocabulary centralised in [glossary.md](glossary.md) ensures consistent messaging.\nKeep tone authoritative, instructional, and encouraging as reflected in both transcripts. *Rationale:* Matches target audience described in video outlines.\nMaintain chronological order in lists (for example, \"Stage 1 -> Stage 2 -> Stage 3\"). *Rationale:* Prevents cognitive dissonance and supports pedagogy.",
      "1. Audit Existing Data & Targets: Understand transcript structure, narration pace, and how timeline annotations map to speech.\nCatalogue every element attribute that must be predicted: `timestamp`, `duration`, `type`, `layer`, `content/description`, `style`, `animation`, `sound`, `action`, optional `context` and `confidence` (see [element_schema.json](element_schema.json)).\nConfirm ingestion coverage for both transcripts and JSON timelines; flag missing sections or inconsistent timestamps."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 60,
      "sentence_count": 1,
      "word_count": 10,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 1.94
    }
  },
  {
    "index": 35,
    "start": 173.52,
    "end": 179.6,
    "duration": 6.079999999999984,
    "text": "Currently, we are generously supported by Eric and Wendy Schmidt, Convergent Research,",
    "guideline_snippets": [
      "8. Maintenance Cadence: Quarterly data refresh to incorporate new annotated videos.\nMonthly error analysis session targeting repeated failure modes (mis-timed SFX, incorrect layer stacking).\nCapture feedback from editors to expand context tags and example catalogues.",
      "6. Evaluation: Quantitative: accuracy/F1 for type classification, MAE for timestamps, BLEU/ROUGE for generated text.\nQualitative: human review against `quality_criteria.md` with focus on narrative fit and layer conflicts.\nStress tests: long speaking segments without elements, rapid-fire sections with many overlays, sections requiring metaphors.",
      "Changelog: All notable changes to the AI planning knowledge base will be documented in this file."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 86,
      "sentence_count": 0,
      "word_count": 12,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 1.97
    }
  },
  {
    "index": 36,
    "start": 179.6,
    "end": 182.0,
    "duration": 2.4000000000000057,
    "text": "and the City of New York.",
    "guideline_snippets": [
      "Changelog: All notable changes to the AI planning knowledge base will be documented in this file.",
      "Recurring Cues: Lists expand rather than reset, so design overlays that append items sequentially.\nCredibility signals (\"Forbes\", \"Inc\", \"Top 1%\") deserve combined overlay plus b-roll emphasis.\nFailure stories lean on darker-toned visuals and restrained SFX; resolve with lighter shots as tone lifts.\nRelationship-building advice pairs well with collaborative or networking imagery.",
      "Element Schema (element_schema.json): This document describes the JSON schema used to define the structure and constraints for all timeline elements generated by the AI planning system. This schema is critical for ensuring consistency, validity, and interoperability of the AI's output with the video rendering pipeline."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 25,
      "sentence_count": 1,
      "word_count": 6,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 2.5
    }
  },
  {
    "index": 37,
    "start": 182.0,
    "end": 184.0,
    "duration": 2.0,
    "text": "We're also, yeah, New York, right?",
    "guideline_snippets": [
      "Layer & Timing Hygiene: Do not overlap full-screen overlays at the same timestamp; offset by at least 0.3 s or merge copy. *Rationale:* Respects the stack hierarchy defined in [element_definitions.md](element_definitions.md#layer-stack).\nWhen the `video` layer is active (b-roll), overlays may occupy full width; otherwise keep margins to protect speaker visibility. *Rationale:* Preserves essential facial expressions.\nEnd SFX before the next major beat and cap overlay visibility at 6-8 s unless the segment is static. *Rationale:* Aligns with [quality_criteria.md](quality_criteria.md#timing) to prevent lingering elements.",
      "Layer Integrity: Only one element per layer at any instant (exceptions: audio layer can stack ambience plus SFX if mix-tested).\nTransitions do not collide with overlays unless intentionally coordinated.",
      "Recurring Cues: Lists expand rather than reset, so design overlays that append items sequentially.\nCredibility signals (\"Forbes\", \"Inc\", \"Top 1%\") deserve combined overlay plus b-roll emphasis.\nFailure stories lean on darker-toned visuals and restrained SFX; resolve with lighter shots as tone lifts.\nRelationship-building advice pairs well with collaborative or networking imagery."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 34,
      "sentence_count": 1,
      "word_count": 6,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 3.0
    }
  },
  {
    "index": 38,
    "start": 184.0,
    "end": 186.0,
    "duration": 2.0,
    "text": "And everything.",
    "guideline_snippets": [
      "3. Feature Engineering: Transcript features: semantic vectors, sentence length, relative position within the video, keyword flags (for example, \"Part 1\", \"mistake\", \"option\").\nAudio/tempo proxies: estimate speaking rate via word counts per window.\nTimeline context: distance to previous/next elements, element type history, context tags (aligned to `context_rules.json`).\nStyle cues: reuse `context` labels to signal metaphors, comparisons, or emphasis needs.\nAsset compatibility: embed catalog metadata (for example, `tags`, `mood`, `intensity`) from `broll_catalog.json` and `sfx_catalog.json` for recommendation scoring.",
      "7. Deployment Checklist: Package preprocessing scripts, model weights, `element_schema.json`, and inference pipeline.\nProvide fallbacks when confidence is low (for example, default overlays or prompts for manual review).\nLog predictions with timestamps and context for downstream auditing.",
      "Type-Specific Requirements (allOf): The schema uses `allOf` to define conditional requirements based on the `type` of the element:\n**`broll`**:\nRequires `description`.\n`layer` must be `video`.\n**`text_overlay`, `text_animation`, `icon`, `section_header`, `emphasis`**:\nRequires `content`.\n`layer` must be `overlay`.\n**`sound_effect`**:\nRequires `sound`.\n`layer` must be `audio`.\n**`effect`**:\nRequires `action` and `duration`.\n`layer` must be `transition`.\n**`speaker_intro`, `achievement_highlight`**:\n`layer` must be `main`.\nThis schema ensures that all AI-generated video plan elements adhere to a strict, well-defined structure, facilitating robust validation and seamless integration into the video production workflow."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 15,
      "sentence_count": 1,
      "word_count": 2,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 1.0
    }
  },
  {
    "index": 39,
    "start": 186.8,
    "end": 192.4,
    "duration": 5.599999999999994,
    "text": "We're also going to apply these tools to intractable chronic diseases, particularly",
    "guideline_snippets": [
      "6. Evaluation: Quantitative: accuracy/F1 for type classification, MAE for timestamps, BLEU/ROUGE for generated text.\nQualitative: human review against `quality_criteria.md` with focus on narrative fit and layer conflicts.\nStress tests: long speaking segments without elements, rapid-fire sections with many overlays, sections requiring metaphors.",
      "8. Maintenance Cadence: Quarterly data refresh to incorporate new annotated videos.\nMonthly error analysis session targeting repeated failure modes (mis-timed SFX, incorrect layer stacking).\nCapture feedback from editors to expand context tags and example catalogues.",
      "Review Workflow: Automated checks: compare predicted elements to ground truth with metrics (F1, MAE).\nHuman review: run through transcripts while watching the plan, verifying that each cue feels natural.\nRegression guardrail: maintain a curated library of \"golden\" plan outputs; new models must meet or exceed their scores."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 83,
      "sentence_count": 0,
      "word_count": 12,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 2.14
    }
  },
  {
    "index": 40,
    "start": 192.4,
    "end": 196.04,
    "duration": 3.6399999999999864,
    "text": "neurological and psychiatric disorders.",
    "guideline_snippets": [
      "Usage: **Training**: Used as structured training data to teach the AI model how to generate valid and effective video plans.\n**Validation**: Provides concrete test cases for validating the AI's output against known good examples.\n**Debugging**: Helps in understanding and debugging unexpected behaviors in the AI's plan generation by comparing outputs to these reference patterns.\nRefer to `knowledge-base/examples/patterns.md` for qualitative descriptions and further context on these editing patterns.",
      "`sound_effect`: **Purpose** - S\u1eed d\u1ee5ng c\u00e1c t\u00edn hi\u1ec7u \u00e2m thanh ng\u1eafn \u0111\u1ec3 nh\u1ea5n m\u1ea1nh c\u00e1c kho\u1ea3nh kh\u1eafc quan tr\u1ecdng, t\u1ea1o c\u1ea3m x\u00fac ho\u1eb7c t\u0103ng c\u01b0\u1eddng tr\u1ea3i nghi\u1ec7m ng\u01b0\u1eddi xem.\n**Layer** - `audio` (l\u1edbp \u00e2m thanh).\n**Key field** - `sound` (tham chi\u1ebfu \u0111\u1ebfn ID \u00e2m thanh trong `sfx_catalog.json`).\n**Common sounds** - `transition_rewind` (chuy\u1ec3n c\u1ea3nh tua l\u1ea1i), `whoosh_standard` (ti\u1ebfng v\u00fat ti\u00eau chu\u1ea9n), `ui_pop` (ti\u1ebfng pop giao di\u1ec7n ng\u01b0\u1eddi d\u00f9ng), `money` (ti\u1ec1n), `success` (th\u00e0nh c\u00f4ng), `fire` (l\u1eeda), `achievement` (th\u00e0nh t\u1ef1u), `crash` (va ch\u1ea1m), `money_loss` (m\u1ea5t ti\u1ec1n), `typing` (g\u00f5 ph\u00edm), `confusion` (b\u1ed1i r\u1ed1i), `expansion` (m\u1edf r\u1ed9ng), `emphasis_ding` (ti\u1ebfng ding nh\u1ea5n m\u1ea1nh), `heartbeat_soft` (ti\u1ebfng tim \u0111\u1eadp nh\u1eb9).\n**Defaults** - Ph\u00e1t m\u1ed9t l\u1ea7n v\u1edbi th\u1eddi l\u01b0\u1ee3ng \u0111\u01b0\u1ee3c x\u00e1c \u0111\u1ecbnh b\u1edfi t\u00e0i s\u1ea3n \u00e2m thanh g\u1ed1c.",
      "6. Evaluation: Quantitative: accuracy/F1 for type classification, MAE for timestamps, BLEU/ROUGE for generated text.\nQualitative: human review against `quality_criteria.md` with focus on narrative fit and layer conflicts.\nStress tests: long speaking segments without elements, rapid-fire sections with many overlays, sections requiring metaphors."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 39,
      "sentence_count": 1,
      "word_count": 4,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 1.1
    }
  },
  {
    "index": 41,
    "start": 196.04,
    "end": 198.52,
    "duration": 2.480000000000018,
    "text": "So why neuropsych, right?",
    "guideline_snippets": [
      "Usage: **Training**: Used as structured training data to teach the AI model how to generate valid and effective video plans.\n**Validation**: Provides concrete test cases for validating the AI's output against known good examples.\n**Debugging**: Helps in understanding and debugging unexpected behaviors in the AI's plan generation by comparing outputs to these reference patterns.\nRefer to `knowledge-base/examples/patterns.md` for qualitative descriptions and further context on these editing patterns.",
      "Feature vs. Benefit: **Pattern**: Icon effect with a subtle sound cue for emphasis.\n**Elements**: `icon` (overlay layer) appearing with a `ui_pop` `sound_effect` (audio layer) when a feature or benefit is introduced.\n**Rationale**: Draws attention to the distinction and reinforces the learning point with an auditory cue.",
      "Changelog: All notable changes to the AI planning knowledge base will be documented in this file."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 25,
      "sentence_count": 1,
      "word_count": 4,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 1.61
    }
  },
  {
    "index": 42,
    "start": 198.52,
    "end": 203.52,
    "duration": 5.0,
    "text": "Emerging evidence, including some of my own work, suggested a lot of what we are calling",
    "guideline_snippets": [
      "Recurring Cues: Lists expand rather than reset, so design overlays that append items sequentially.\nCredibility signals (\"Forbes\", \"Inc\", \"Top 1%\") deserve combined overlay plus b-roll emphasis.\nFailure stories lean on darker-toned visuals and restrained SFX; resolve with lighter shots as tone lifts.\nRelationship-building advice pairs well with collaborative or networking imagery.",
      "6. Evaluation: Quantitative: accuracy/F1 for type classification, MAE for timestamps, BLEU/ROUGE for generated text.\nQualitative: human review against `quality_criteria.md` with focus on narrative fit and layer conflicts.\nStress tests: long speaking segments without elements, rapid-fire sections with many overlays, sections requiring metaphors.",
      "Review Workflow: Automated checks: compare predicted elements to ground truth with metrics (F1, MAE).\nHuman review: run through transcripts while watching the plan, verifying that each cue feels natural.\nRegression guardrail: maintain a curated library of \"golden\" plan outputs; new models must meet or exceed their scores."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 88,
      "sentence_count": 0,
      "word_count": 16,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 3.2
    }
  },
  {
    "index": 43,
    "start": 203.52,
    "end": 208.08,
    "duration": 4.560000000000002,
    "text": "psychiatric disorders are actually immune diseases.",
    "guideline_snippets": [
      "Coverage: Every major section (see `videos/*.md`) contains at least one supporting visual element.\nNo unaddressed transcript peaks: stories, statistics, frameworks, and comparisons must trigger at least a text overlay or b-roll.",
      "6. Evaluation: Quantitative: accuracy/F1 for type classification, MAE for timestamps, BLEU/ROUGE for generated text.\nQualitative: human review against `quality_criteria.md` with focus on narrative fit and layer conflicts.\nStress tests: long speaking segments without elements, rapid-fire sections with many overlays, sections requiring metaphors.",
      "Purpose: The `patterns.json` file provides machine-readable examples that complement the qualitative descriptions in `patterns.md`. It helps the AI model learn:\n**Correct sequencing**: How elements like `text_overlay`, `broll`, and `sound_effect` should follow each other.\n**Contextual application**: When to use specific styles, animations, or sound effects based on the narrative context.\n**Constraint adherence**: Examples that demonstrate compliance with rules defined in `element_schema.json` and `motion_rules.json`."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 51,
      "sentence_count": 1,
      "word_count": 6,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 1.32
    }
  },
  {
    "index": 44,
    "start": 208.08,
    "end": 215.04,
    "duration": 6.9599999999999795,
    "text": "So for example, the immune system stores a memory of psychological stress and is involved",
    "guideline_snippets": [
      "Purpose: The `patterns.json` file provides machine-readable examples that complement the qualitative descriptions in `patterns.md`. It helps the AI model learn:\n**Correct sequencing**: How elements like `text_overlay`, `broll`, and `sound_effect` should follow each other.\n**Contextual application**: When to use specific styles, animations, or sound effects based on the narrative context.\n**Constraint adherence**: Examples that demonstrate compliance with rules defined in `element_schema.json` and `motion_rules.json`.",
      "Usage: **Training**: Used as structured training data to teach the AI model how to generate valid and effective video plans.\n**Validation**: Provides concrete test cases for validating the AI's output against known good examples.\n**Debugging**: Helps in understanding and debugging unexpected behaviors in the AI's plan generation by comparing outputs to these reference patterns.\nRefer to `knowledge-base/examples/patterns.md` for qualitative descriptions and further context on these editing patterns.",
      "6. Evaluation: Quantitative: accuracy/F1 for type classification, MAE for timestamps, BLEU/ROUGE for generated text.\nQualitative: human review against `quality_criteria.md` with focus on narrative fit and layer conflicts.\nStress tests: long speaking segments without elements, rapid-fire sections with many overlays, sections requiring metaphors."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 89,
      "sentence_count": 0,
      "word_count": 15,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 2.16
    }
  },
  {
    "index": 45,
    "start": 215.08,
    "end": 220.8,
    "duration": 5.719999999999999,
    "text": "in stress resilience, and that a lot of cases of schizophrenia are actually turning out to",
    "guideline_snippets": [
      "6. Evaluation: Quantitative: accuracy/F1 for type classification, MAE for timestamps, BLEU/ROUGE for generated text.\nQualitative: human review against `quality_criteria.md` with focus on narrative fit and layer conflicts.\nStress tests: long speaking segments without elements, rapid-fire sections with many overlays, sections requiring metaphors.",
      "Coverage: Every major section (see `videos/*.md`) contains at least one supporting visual element.\nNo unaddressed transcript peaks: stories, statistics, frameworks, and comparisons must trigger at least a text overlay or b-roll.",
      "8. Maintenance Cadence: Quarterly data refresh to incorporate new annotated videos.\nMonthly error analysis session targeting repeated failure modes (mis-timed SFX, incorrect layer stacking).\nCapture feedback from editors to expand context tags and example catalogues."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 90,
      "sentence_count": 0,
      "word_count": 16,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 2.8
    }
  },
  {
    "index": 46,
    "start": 220.8,
    "end": 224.84,
    "duration": 4.039999999999992,
    "text": "be driven by underlying autoimmune disease.",
    "guideline_snippets": [
      "Review Workflow: Automated checks: compare predicted elements to ground truth with metrics (F1, MAE).\nHuman review: run through transcripts while watching the plan, verifying that each cue feels natural.\nRegression guardrail: maintain a curated library of \"golden\" plan outputs; new models must meet or exceed their scores.",
      "Asset Catalog Integration: This document details how asset catalogs are integrated into the video automation pipeline. It ensures that the AI model's outputs can be directly mapped to available production assets, guaranteeing the generation of actionable and feasible video plans.\nConnect model outputs with available production assets to guarantee actionable plans.",
      "9. Risk Controls: Mitigate noisy timestamps by smoothing predictions and enforcing minimum separation thresholds defined in `motion_rules.json`.\nPrevent style drift by constraining `style` predictions to allowed set per brand guide.\nDocument assumptions about transcript accuracy; include fallbacks when speech deviates from script."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 43,
      "sentence_count": 1,
      "word_count": 6,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 1.49
    }
  },
  {
    "index": 47,
    "start": 224.84,
    "end": 229.72,
    "duration": 4.8799999999999955,
    "text": "And the shingles vaccine might prevent dementia.",
    "guideline_snippets": [
      "9. Risk Controls: Mitigate noisy timestamps by smoothing predictions and enforcing minimum separation thresholds defined in `motion_rules.json`.\nPrevent style drift by constraining `style` predictions to allowed set per brand guide.\nDocument assumptions about transcript accuracy; include fallbacks when speech deviates from script.",
      "Sound Effects: Pair new terms or section changes with subtle SFX (`ui_pop`, `whoosh_standard`). *Rationale:* Audio cues boost recall and correspond to assets in `sfx_catalog.json`.\nReserve `money`, `success`, or `achievement` sounds for monetary claims or milestones. *Rationale:* Prevents semantic drift and keeps the acoustic palette purposeful.\nUse `typing` sounds only when animations or narration mention writing or typing. *Rationale:* Maintains cohesive audiovisual storytelling.",
      "Motion & Transitions: Trigger `zoom_in` on turning points or emphasised statements (\"this is the key\", \"here's the mistake\"). *Rationale:* Heightens focus; validated by examples in [examples/patterns.json](examples/patterns.json).\nDeploy `zoom_out` to release tension or provide wider context after a focal point. *Rationale:* Restores viewer orientation.\nSpace identical effects by at least 0.5 s unless narrative urgency demands otherwise. *Rationale:* Enforced by `motion_rules.json`; avoids viewer fatigue (see negative cases in `patterns.json`)."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 48,
      "sentence_count": 1,
      "word_count": 7,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 1.43
    }
  },
  {
    "index": 48,
    "start": 229.72,
    "end": 232.56,
    "duration": 2.8400000000000034,
    "text": "And there's also everything that we saw during the pandemic, right?",
    "guideline_snippets": [
      "Changelog: All notable changes to the AI planning knowledge base will be documented in this file.",
      "Coverage: Every major section (see `videos/*.md`) contains at least one supporting visual element.\nNo unaddressed transcript peaks: stories, statistics, frameworks, and comparisons must trigger at least a text overlay or b-roll.",
      "Layer Stack: `main` - primary camera feed (baseline footage).\n`video` - b-roll replacing or augmenting the main layer.\n`overlay` - text, icons, and animated graphics.\n`audio` - sound effects mixed with narration.\n`transition` - temporary visual effects used between clips.\nRespect layer priority to avoid stacking conflicts; only one dominant element per layer at a time."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 67,
      "sentence_count": 1,
      "word_count": 11,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 3.87
    }
  },
  {
    "index": 49,
    "start": 232.56,
    "end": 238.56,
    "duration": 6.0,
    "text": "COVID causing de novo psychiatric disorders, long COVID.",
    "guideline_snippets": [
      "2. Preprocess & Align: Merge transcript text with timeline entries by timestamp, producing windows (5-10 seconds) that give sufficient context.\nNormalize timestamps to seconds for easier maths and modelling.\nSegment transcripts into sentences or clauses; tokenise and build embeddings (Sentence-BERT or equivalent).\nEncode video elements: one-hot for categorical fields, embeddings for text-heavy attributes (`description`, `content`); ensure categorical vocab matches [element_schema.json](element_schema.json).",
      "Coverage: Every major section (see `videos/*.md`) contains at least one supporting visual element.\nNo unaddressed transcript peaks: stories, statistics, frameworks, and comparisons must trigger at least a text overlay or b-roll.",
      "6. Evaluation: Quantitative: accuracy/F1 for type classification, MAE for timestamps, BLEU/ROUGE for generated text.\nQualitative: human review against `quality_criteria.md` with focus on narrative fit and layer conflicts.\nStress tests: long speaking segments without elements, rapid-fire sections with many overlays, sections requiring metaphors."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 56,
      "sentence_count": 1,
      "word_count": 8,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 1.33
    }
  },
  {
    "index": 50,
    "start": 238.56,
    "end": 244.24,
    "duration": 5.680000000000007,
    "text": "So really, we are in the middle of an immunotherapy revolution for cancer, right?",
    "guideline_snippets": [
      "Coverage: Every major section (see `videos/*.md`) contains at least one supporting visual element.\nNo unaddressed transcript peaks: stories, statistics, frameworks, and comparisons must trigger at least a text overlay or b-roll.",
      "Recurring Cues: Lists expand rather than reset, so design overlays that append items sequentially.\nCredibility signals (\"Forbes\", \"Inc\", \"Top 1%\") deserve combined overlay plus b-roll emphasis.\nFailure stories lean on darker-toned visuals and restrained SFX; resolve with lighter shots as tone lifts.\nRelationship-building advice pairs well with collaborative or networking imagery.",
      "Content Accuracy: On-screen text mirrors transcript wording or summarises it accurately without introducing new facts.\nB-roll descriptions are actionable for editors (clear subject, action, mood).\nSound choices reinforce the message rather than distract."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 81,
      "sentence_count": 1,
      "word_count": 14,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 2.46
    }
  },
  {
    "index": 51,
    "start": 244.24,
    "end": 252.44,
    "duration": 8.199999999999989,
    "text": "Some cancers that had 90% mortality rates 50 years ago now have 90% cure rates.",
    "guideline_snippets": [
      "Coverage: Every major section (see `videos/*.md`) contains at least one supporting visual element.\nNo unaddressed transcript peaks: stories, statistics, frameworks, and comparisons must trigger at least a text overlay or b-roll.",
      "Recurring Cues: `highlighted_background` remains the default text treatment throughout; maintain consistency unless context demands variation.\nComparisons thrive on side-by-side overlays (for example, \"Feature | Benefit\", \"Organic | Paid\"); ensure columns stay balanced.\nIntroduce new terminology with matching SFX and maintain momentum with responsive motion cues.\nReserve zoom chains for genuine emphasis; interleave with fades or slides to give viewers recovery time.",
      "Content Accuracy: On-screen text mirrors transcript wording or summarises it accurately without introducing new facts.\nB-roll descriptions are actionable for editors (clear subject, action, mood).\nSound choices reinforce the message rather than distract."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 79,
      "sentence_count": 1,
      "word_count": 15,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 1.83
    }
  },
  {
    "index": 52,
    "start": 252.44,
    "end": 260.96,
    "duration": 8.519999999999982,
    "text": "What if we could extend that revolution to autoimmunity, diabetes, dementia, depression?",
    "guideline_snippets": [
      "Changelog: All notable changes to the AI planning knowledge base will be documented in this file.",
      "9. Risk Controls: Mitigate noisy timestamps by smoothing predictions and enforcing minimum separation thresholds defined in `motion_rules.json`.\nPrevent style drift by constraining `style` predictions to allowed set per brand guide.\nDocument assumptions about transcript accuracy; include fallbacks when speech deviates from script.",
      "Review Workflow: Automated checks: compare predicted elements to ground truth with metrics (F1, MAE).\nHuman review: run through transcripts while watching the plan, verifying that each cue feels natural.\nRegression guardrail: maintain a curated library of \"golden\" plan outputs; new models must meet or exceed their scores."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 88,
      "sentence_count": 1,
      "word_count": 12,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 1.41
    }
  },
  {
    "index": 53,
    "start": 260.96,
    "end": 267.48,
    "duration": 6.520000000000039,
    "text": "Maybe in 15 years, if we can learn what the immune system already knows, we could take",
    "guideline_snippets": [
      "Changelog: All notable changes to the AI planning knowledge base will be documented in this file.",
      "2. Preprocess & Align: Merge transcript text with timeline entries by timestamp, producing windows (5-10 seconds) that give sufficient context.\nNormalize timestamps to seconds for easier maths and modelling.\nSegment transcripts into sentences or clauses; tokenise and build embeddings (Sentence-BERT or equivalent).\nEncode video elements: one-hot for categorical fields, embeddings for text-heavy attributes (`description`, `content`); ensure categorical vocab matches [element_schema.json](element_schema.json).",
      "Training Pipeline Blueprint: This document serves as the authoritative blueprint for the AI video plan generation training pipeline. It details the end-to-end process, from data preparation and feature engineering to model strategy, training routines, evaluation, and deployment. This blueprint is derived from the `AI_Training_Plan.md` (Vietnamese source) and should be used as the primary reference when preparing datasets, developing new models, or evolving the existing model stack.\nDerived from `AI_Training_Plan.md` (Vietnamese source). Use this as the authoritative workflow when preparing datasets or evolving the model stack."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 86,
      "sentence_count": 0,
      "word_count": 17,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 2.61
    }
  },
  {
    "index": 54,
    "start": 267.48,
    "end": 271.08,
    "duration": 3.599999999999966,
    "text": "the chronic out of chronic disease.",
    "guideline_snippets": [
      "6. Evaluation: Quantitative: accuracy/F1 for type classification, MAE for timestamps, BLEU/ROUGE for generated text.\nQualitative: human review against `quality_criteria.md` with focus on narrative fit and layer conflicts.\nStress tests: long speaking segments without elements, rapid-fire sections with many overlays, sections requiring metaphors.",
      "8. Maintenance Cadence: Quarterly data refresh to incorporate new annotated videos.\nMonthly error analysis session targeting repeated failure modes (mis-timed SFX, incorrect layer stacking).\nCapture feedback from editors to expand context tags and example catalogues.",
      "Usage: **Training**: Used as structured training data to teach the AI model how to generate valid and effective video plans.\n**Validation**: Provides concrete test cases for validating the AI's output against known good examples.\n**Debugging**: Helps in understanding and debugging unexpected behaviors in the AI's plan generation by comparing outputs to these reference patterns.\nRefer to `knowledge-base/examples/patterns.md` for qualitative descriptions and further context on these editing patterns."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 35,
      "sentence_count": 1,
      "word_count": 6,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 1.67
    }
  },
  {
    "index": 55,
    "start": 271.08,
    "end": 272.08,
    "duration": 1.0,
    "text": "Thank you.",
    "guideline_snippets": [
      "Type-Specific Requirements (allOf): The schema uses `allOf` to define conditional requirements based on the `type` of the element:\n**`broll`**:\nRequires `description`.\n`layer` must be `video`.\n**`text_overlay`, `text_animation`, `icon`, `section_header`, `emphasis`**:\nRequires `content`.\n`layer` must be `overlay`.\n**`sound_effect`**:\nRequires `sound`.\n`layer` must be `audio`.\n**`effect`**:\nRequires `action` and `duration`.\n`layer` must be `transition`.\n**`speaker_intro`, `achievement_highlight`**:\n`layer` must be `main`.\nThis schema ensures that all AI-generated video plan elements adhere to a strict, well-defined structure, facilitating robust validation and seamless integration into the video production workflow.",
      "Synchronisation Notes: Multiple elements can share the same timestamp; treat them as simultaneous but respect layer ordering.\nTransitions (`effect`) should begin before or exactly with the visual/text they introduce.\nUse `style` values to keep branding consistent (default: `highlighted_background` for critical callouts).\nPopulate `context` to support catalog lookups (see [asset_catalogs.md](asset_catalogs.md)).\nRecord `confidence` when available to support downstream thresholding (optional field in `element_schema.json`).",
      "Required Fields for All Elements: `timestamp` (number): Th\u1eddi gian xu\u1ea5t hi\u1ec7n c\u1ee7a ph\u1ea7n t\u1eed t\u00ednh b\u1eb1ng gi\u00e2y k\u1ec3 t\u1eeb khi video b\u1eaft \u0111\u1ea7u. Ph\u1ea3i l\u00e0 s\u1ed1 kh\u00f4ng \u00e2m.\n`type` (string): \u0110\u1ecbnh danh lo\u1ea1i ph\u1ea7n t\u1eed (v\u00ed d\u1ee5: `broll`, `text_overlay`, `sound_effect`).\n`layer` (string): L\u1edbp hi\u1ec3n th\u1ecb trong ng\u0103n x\u1ebfp t\u1ed5ng h\u1ee3p."
    ],
    "plan_elements": [],
    "encoded_features": {
      "num_elements": 0,
      "avg_element_duration": 0.0
    },
    "text_features": {
      "text_length": 10,
      "sentence_count": 1,
      "word_count": 2,
      "relative_position": 0.0,
      "has_keyword_t\u01b0\u01a1ng_lai": false,
      "has_keyword_c\u00f4ng_ngh\u1ec7": false,
      "has_keyword_ai": false
    },
    "audio_features": {
      "speaking_rate_wps": 2.0
    }
  }
]